{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupiter done by Julien Pelegri & Elouan Raymond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multy linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel (r'data_regression.xlsx')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "boxplot = df.boxplot(column=['X0', 'X1','X2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.X0.plot.hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.X1.plot.hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.X2.plot.hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the dataset using a scaler is useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.suptitle('Y output for X1 input', fontsize=16)\n",
    "plt.scatter(df['X1'], df['Y'])\n",
    "plt.xlabel('X1', fontsize=14)\n",
    "plt.ylabel('Y', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# color it\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1 , X2 , X3 have a really low correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that a Lasso would only consider X1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data vizualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df['X0'], y=df['Y'], data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df['X1'], y=df['Y'], data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df['X2'], y=df['Y'], data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot('X1','X2', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots\n",
    "sns.pairplot(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tough to find a pattern cause I don't have enough data, but still can find some symetries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 +\\ldots + \\hat\\beta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "\n",
    "X = df[['X0', 'X1', 'X2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting response variable to y\n",
    "\n",
    "y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_state is the seed used by the random number generator, it can be any integer.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 ,test_size = 0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm          # Importing statsmodels\n",
    "X_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n",
    "# create a first fitted model\n",
    "fitting = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our first linear model\n",
    "print(fitting.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result are satisfying knowing the dataset and the correlation between variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for fun\n",
    "fitting.predict([[1,12,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can not do MSE cause the data set is too samll (doesn't make a lot of sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#y_true = ...\n",
    "#y_pred = ...\n",
    "\n",
    "#mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More classic linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instanciate Linear model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train Linear Model\n",
    "model.fit(df[['X0', \"X1\", \"X2\"]], df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the model's inner parameters slope(coef_) and intercept(intercept_)\n",
    "slope_a, intercept_b = model.coef_, model.intercept_ \n",
    "\n",
    "print(slope_a)\n",
    "print(intercept_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear combination of initial dataset columns\n",
    "\n",
    "## ùëå = ùõΩ + ùõΩ0ùëã0 + ùõΩ1ùëã1 + ùõΩ2ùëã2 , with the model above we can get all ùõΩi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ùõΩ = 1.42\n",
    "- ùõΩ0 = 0\n",
    "- ùõΩ1 = 7.17\n",
    "- ùõΩ2 = -2.25\n",
    "\n",
    "- ùõΩ0 is not taking into account, it is like an outilier for the model. Which is coherent when looking at the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to implement the multi linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[['X0', \"X1\", \"X2\"]]\n",
    "y = df['Y']\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Import the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instanciate the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the Training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the Testing data\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.X0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for X1 (cause it is the more relevant input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the model attributes w(coef_) and b(intercept_)\n",
    "w, b = model.coef_, model.intercept_ \n",
    "\n",
    "# Defining the line of best fit equation\n",
    "best_fit = w * X + b\n",
    "\n",
    "# Plot!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X.X1, y, alpha=0.9)\n",
    "plt.plot(X, best_fit, c=\"red\")\n",
    "plt.title('Scatter plot + Best Fit')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing other types of regressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "ridge = Ridge(alpha=100).fit(X, y)\n",
    "lasso = Lasso(alpha=100).fit(X, y)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"coef_linreg\": pd.Series(linreg.coef_, index = X.columns),\n",
    "    \"coef_ridge\": pd.Series(ridge.coef_, index = X.columns),\n",
    "    \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results are not commun due to the dataset distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I didn't have expected this kind of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the error of the model for different values of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "alphas = [0, 0.01, 0.05, 0.1, 0.5, 1, 10, 50]\n",
    "r2_cv = []\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha = alpha)\n",
    "    r2_cv.append(cross_validate(model, X, y, cv=5)['test_score'].mean())\n",
    "r2_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "alphas = [0, 0.01, 0.05, 0.1, 0.5, 1, 10, 50]\n",
    "r2_cv = []\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha = alpha)\n",
    "    r2_cv.append(cross_validate(model, X, y, cv=5)['test_score'].mean())\n",
    "r2_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso and Ridge are not working well for this kind of problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient & Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have play around with some linear prediction using various models lets rebuild some gradient calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuilding a basic gradient descent :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(x,a,b):\n",
    "    y_pred = a*x + b\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(x,y,a,b):\n",
    "    y_pred = hypothesis(x,a,b)\n",
    "    loss = np.sum((y-y_pred) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} )\\times x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives(x,y,a,b):\n",
    "    y_pred = hypothesis(x,a,b)\n",
    "    derivative_a = np.sum(-2*(y-y_pred)*x)\n",
    "    derivative_b = np.sum(-2*(y-y_pred))\n",
    "    return derivative_a, derivative_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = derivative \\times learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(derivative_a,derivative_b,learning_rate = 0.01):\n",
    "    step_a = derivative_a*learning_rate\n",
    "    step_b = derivative_b*learning_rate\n",
    "    return step_a, step_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update(a, step_a, b, step_b):\n",
    "    updated_a = a - step_a\n",
    "    updated_b = b - step_b\n",
    "    return updated_a , updated_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider convergence to be 150 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['X1']\n",
    "y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a and b\n",
    "a = 1\n",
    "b = 0\n",
    "loss_history = []\n",
    "\n",
    "# Loop through steps to perform Gradient Descent\n",
    "for epoch in range(150):\n",
    "    \n",
    "    # Compute Loss at each Epoch and append to loss_history\n",
    "    loss_epoch = loss(x,y,a,b)\n",
    "    loss_history.append(loss_epoch)\n",
    "    \n",
    "    # Compute the Derivates \n",
    "    derivative_a, derivative_b = derivatives(x,y,a,b)\n",
    "    \n",
    "    # Compute Steps\n",
    "    step_a, step_b = steps(derivative_a,derivative_b,learning_rate = 0.01)\n",
    "    \n",
    "    # Compute updated parameters\n",
    "    updated_a, updated_b = parameter_update(a,step_a,b,step_b)\n",
    "    \n",
    "    # Set updated parameters for new epoch\n",
    "    a = updated_a\n",
    "    b = updated_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a nice convergence after 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les librairies n√©cessaires\n",
    "import numpy as np\n",
    "import random \n",
    "from random import sample \n",
    "from numpy.random import randint\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "\n",
    "data = np.loadtxt('data_regression.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic gradient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "n_var = len(data[0])-1\n",
    "\n",
    "# D√©finition des tableaux\n",
    "X = data[:,0:3]\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "Y = data[:,3]\n",
    "print(\"Y:\")\n",
    "print(Y)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparam√®tres\n",
    "ALPHA = 0.0001  #  taux d'apprentissage\n",
    "EPOCHS = 500000  #  number of iterations to perform gradient descent\n",
    "EPS = 0.4  #  Epsilon\n",
    "beta_hat = np.zeros(n_var) # parametres initiaux\n",
    "\n",
    "MSE = np.zeros(EPOCHS+1)\n",
    "MSE[0] = (np.sum((Y - sum(np.transpose(X*beta_hat)))**2))/n\n",
    "\n",
    "\n",
    "# Descente du gradient\n",
    "i = 0\n",
    "while MSE[i] > EPS and i < EPOCHS:\n",
    "    i += 1\n",
    "    # pr√©diction de Y\n",
    "    Y_pred = sum(np.transpose(X*beta_hat))\n",
    "\n",
    "    for j in range(0,n_var):\n",
    "        beta_hat[j] = beta_hat[j] + ALPHA * (2*np.sum((Y-Y_pred)*X[:,j]))/n\n",
    "\n",
    "    MSE[i] = (np.sum((Y - Y_pred)**2))/n\n",
    "\n",
    "print(\"Iteration finale:\",i)\n",
    "print(\"MSE par descente du gradient:\",MSE[i])\n",
    "\n",
    "beta_hat = np.dot(np.linalg.inv(np.dot(X.T,X)+0.00001*np.random.rand(n_var, n_var)),np.dot(X.T,Y))\n",
    "\n",
    "MSE2 = sum((Y - sum(np.transpose(X*beta_hat)))**2)/n\n",
    "print(\"MSE par m√©thode analytique:\",MSE2)\n",
    "\n",
    "# Configuration du plot\n",
    "x_axis = np.arange(0, i)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x_axis,MSE[0:i])\n",
    "plt.title(\"MSE en fonction du nombre d'it√©ration (gradient analytique)\")\n",
    "plt.xlabel(\"Nombre d'it√©ration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Partie Gradient num√©rique\n",
    "# Hyperparam√®tres\n",
    "D_ALPHA = 0.001\n",
    "beta_hat = np.zeros(n_var) # parametres initiaux\n",
    "\n",
    "MSE = np.zeros(EPOCHS+1)\n",
    "MSE[0] = (np.sum((Y - sum(np.transpose(X*beta_hat)))**2))/n\n",
    "\n",
    "\n",
    "# Descente du gradient\n",
    "i = 0\n",
    "while MSE[i] > EPS and i < EPOCHS:\n",
    "    i += 1\n",
    "    # pr√©diction de Y\n",
    "    Y_pred = sum(np.transpose(X*beta_hat))\n",
    "\n",
    "    for j in range(0,n_var):\n",
    "        mse1 = (np.sum((Y - Y_pred)**2))/n\n",
    "        beta_hat_temp = beta_hat.copy()\n",
    "        beta_hat_temp[j] += D_ALPHA\n",
    "        Y_pred_temp = sum(np.transpose(X*beta_hat_temp))\n",
    "        mse2 = (np.sum((Y - Y_pred_temp)**2))/n\n",
    "\n",
    "\n",
    "        beta_hat[j] = beta_hat[j] - ALPHA * (mse2-mse1)/D_ALPHA\n",
    "\n",
    "    MSE[i] = (np.sum((Y - Y_pred)**2))/n\n",
    "\n",
    "print(\"Iteration finale:\",i)\n",
    "print(\"MSE par d√©scente du gradient num√©rique:\",MSE[i])\n",
    "\n",
    "# Configuration du plot\n",
    "x_axis = np.arange(0, i)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x_axis,MSE[0:i])\n",
    "plt.title(\"MSE en fonction du nombre d'it√©ration (gradient num√©rique)\")\n",
    "plt.xlabel(\"Nombre d'it√©ration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient stochastic : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "n_var = len(data[0])-1\n",
    "\n",
    "# D√©finition des tableaux\n",
    "X = data[:,0:3]\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "Y = data[:,3]\n",
    "print(\"Y:\")\n",
    "print(Y)\n",
    "\n",
    "# Hyperparam√®tres\n",
    "R = 2 # Nombre de variables √† consid√©rer pour le gradient stochastique\n",
    "ALPHA = 0.0001  #  taux d'apprentissage\n",
    "EPOCHS = 500000  #  number of iterations to perform gradient descent\n",
    "EPS = 0.5  #  Epsilon\n",
    "beta_hat = np.zeros(n_var) # parametres initiaux\n",
    "\n",
    "MSE = np.zeros(EPOCHS+1)\n",
    "MSE[0] = (np.sum((Y - sum(np.transpose(X*beta_hat)))**2))/n\n",
    "\n",
    "\n",
    "# Descente du gradient\n",
    "i = 0\n",
    "while MSE[i] > EPS and i < EPOCHS:\n",
    "    i += 1\n",
    "    # pr√©diction de Y\n",
    "    liste = random.sample(list(np.arange(0, n_var)),R)\n",
    "    Y_pred = sum(np.transpose(X*beta_hat))\n",
    "\n",
    "    for j in liste:\n",
    "        beta_hat[j] = beta_hat[j] + ALPHA * (2*np.sum((Y-Y_pred)*X[:,j]))/n\n",
    "\n",
    "    MSE[i] = (np.sum((Y - Y_pred)**2))/n\n",
    "\n",
    "print(\"Iteration finale:\",i)\n",
    "print(\"MSE par descente du gradient:\",MSE[i])\n",
    "\n",
    "beta_hat = np.dot(np.linalg.inv(np.dot(X.T,X)+0.00001*np.random.rand(n_var, n_var)),np.dot(X.T,Y))\n",
    "\n",
    "MSE2 = sum((Y - sum(np.transpose(X*beta_hat)))**2)/n\n",
    "print(\"MSE par m√©thode analytique:\",MSE2)\n",
    "\n",
    "# Configuration du plot\n",
    "x_axis = np.arange(0, i)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x_axis,MSE[0:i])\n",
    "plt.title(\"MSE en fonction du nombre d'it√©ration (gradient analytique)\")\n",
    "plt.xlabel(\"Nombre d'it√©ration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Partie Gradient num√©rique\n",
    "# Hyperparam√®tres\n",
    "D_ALPHA = 0.01\n",
    "beta_hat = np.zeros(n_var) # parametres initiaux\n",
    "\n",
    "MSE = np.zeros(EPOCHS+1)\n",
    "MSE[0] = (np.sum((Y - sum(np.transpose(X*beta_hat)))**2))/n\n",
    "\n",
    "\n",
    "# Descente du gradient\n",
    "i = 0\n",
    "while MSE[i] > EPS and i < EPOCHS:\n",
    "    i += 1\n",
    "    # pr√©diction de Y\n",
    "    liste = random.sample(list(np.arange(0, n_var)),R)\n",
    "    Y_pred = sum(np.transpose(X*beta_hat))\n",
    "\n",
    "    for j in liste:\n",
    "        mse1 = (np.sum((Y - Y_pred)**2))/n\n",
    "        beta_hat_temp = beta_hat.copy()\n",
    "        beta_hat_temp[j] += D_ALPHA\n",
    "        Y_pred_temp = sum(np.transpose(X*beta_hat_temp))\n",
    "        mse2 = (np.sum((Y - Y_pred_temp)**2))/n\n",
    "\n",
    "\n",
    "        beta_hat[j] = beta_hat[j] - ALPHA * (mse2-mse1)/D_ALPHA\n",
    "\n",
    "    MSE[i] = (np.sum((Y - Y_pred)**2))/n\n",
    "\n",
    "print(\"Iteration finale:\",i)\n",
    "print(\"MSE par descente du gradient num√©rique:\",MSE[i])\n",
    "\n",
    "# Configuration du plot\n",
    "x_axis = np.arange(0, i)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x_axis,MSE[0:i])\n",
    "plt.title(\"MSE en fonction du nombre d'it√©ration (gradient num√©rique)\")\n",
    "plt.xlabel(\"Nombre d'it√©ration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF JUPITER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
