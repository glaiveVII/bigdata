{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Julien Pelegri hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : play around with hash funcitons, build some and encode some .txt file. Try to optimize time and space complexity and have as few collision as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    "- 1/ Play around\n",
    "- 2/ Building a hash function\n",
    "- 3/ Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hash.png\",width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/ Play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max size of words I can get :  9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "print(\"The max size of words I can get : \" , six.MAXSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Hash test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for 181 is: 181\n",
      "Hash for 181.23 is: 530343892119126197\n",
      "Hash for Python is: 8433711297258875460\n"
     ]
    }
   ],
   "source": [
    "# hash for integer unchanged\n",
    "print('Hash for 181 is:', hash(181))\n",
    "\n",
    "# hash for decimal\n",
    "print('Hash for 181.23 is:',hash(181.23))\n",
    "\n",
    "# hash for string\n",
    "print('Hash for Python is:', hash('Python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the .txt file size with the wc command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J'ai cree un word.txt file that has only 500 words to test faster\n",
    "\n",
    "wc word.txt\n",
    "\n",
    ">> 500\n",
    "\n",
    "wc word2.txt\n",
    "\n",
    ">> 235886\n",
    "\n",
    "wc texte_Shakespeare.txt\n",
    "\n",
    ">> 22906\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file first "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have create a smaller .txt file with only a to go faster when testing since the file is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the file:word2.txt\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "inputFile = input(\"Enter the name of the file:\")\n",
    "\n",
    "f=open(inputFile, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='word2.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "print(f)\n",
    "txt = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now read the file line per line :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(file):\n",
    "    words =[] #Liste pour stocker les diff√©rentes lignes\n",
    "    # Use str.rstrip() to remove a trailing newline \n",
    "    countword = 0\n",
    "\n",
    "    for line in f:\n",
    "        countword += 1\n",
    "       # print(len(words))\n",
    "        words.append(line.rstrip(\"\\n\"))\n",
    "    save = words\n",
    "    return(countword, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = count(f)\n",
    "words = x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235886\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235886\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zoogeographic', 'zoogeographical', 'zoogeographically', 'zoogeography', 'zoogeological', 'zoogeologist', 'zoogeology', 'zoogloea', 'zoogloeal', 'zoogloeic', 'zoogonic', 'zoogonidium', 'zoogonous', 'zoogony', 'zoograft', 'zoografting', 'zoographer', 'zoographic', 'zoographical', 'zoographically', 'zoographist', 'zoography', 'zooid', 'zooidal', 'zooidiophilous', 'zooks', 'zoolater', 'zoolatria', 'zoolatrous', 'zoolatry', 'zoolite', 'zoolith', 'zoolithic', 'zoolitic', 'zoologer', 'zoologic', 'zoological', 'zoologically', 'zoologicoarchaeologist', 'zoologicobotanical', 'zoologist', 'zoologize', 'zoology', 'zoom', 'zoomagnetic', 'zoomagnetism', 'zoomancy', 'zoomania', 'zoomantic', 'zoomantist', 'zoomastigina', 'zoomastigoda', 'zoomechanical', 'zoomechanics', 'zoomelanin', 'zoometric', 'zoometry', 'zoomimetic', 'zoomimic', 'zoomorph', 'zoomorphic', 'zoomorphism', 'zoomorphize', 'zoomorphy', 'zoon', 'zoonal', 'zoonerythrin', 'zoonic', 'zoonist', 'zoonite', 'zoonitic', 'zoonomia', 'zoonomic', 'zoonomical', 'zoonomist', 'zoonomy', 'zoonosis', 'zoonosologist', 'zoonosology', 'zoonotic', 'zoons', 'zoonule', 'zoopaleontology', 'zoopantheon', 'zooparasite', 'zooparasitic', 'zoopathological', 'zoopathologist', 'zoopathology', 'zoopathy', 'zooperal', 'zooperist', 'zoopery', 'zoophaga', 'zoophagan', 'zoophagineae', 'zoophagous', 'zoopharmacological', 'zoopharmacy', 'zoophile']\n"
     ]
    }
   ],
   "source": [
    "print(words[235500:235600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x[1][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now going through words array and hash it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha3-384', 'blake2s', 'sha3_224', 'mdc2', 'sm3', 'shake128', 'sha3-224', 'blake2b', 'sha512-224', 'shake256', 'md4', 'sha3_512', 'sha1', 'sha3-256', 'sha384', 'sha256', 'md5', 'blake2s256', 'sha512-256', 'sha512', 'shake_128', 'shake_256', 'sha3-512', 'whirlpool', 'blake2b512', 'md5-sha1', 'sha224', 'ripemd160', 'sha3_256', 'sha3_384'}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "print(hashlib.algorithms_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic python hash : use the hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4171325285679281702, -4171325285679281702, 241257824951959063, 4836067789152381503, 3902140602386838048, -7474235662673059313, 9141986221852515719, 6225106884422236343, -4794784451985316634, 9189272364536265254, 5458349250009168514, -7053676826261117251, -460387931375362167, 8043536204027204652, 2175191675575533109, 563500484603246902, -4885731984279105941, -5316575686257715355, -3590467628694245037, 5920740019567254019, 4765273707415591764, -5699296906277035707, -4716505829752630546, -2794321411084985558, 7305931724651824063, 2433029802783938724, -8959206682406745050, -5282633866437803901, -8920583246873706963, -785746101362605489, 6032376243553454593, 6120749525202700992, 7699598248885906366, 3145877043908382890, -2912563321880091547, 5819924712330762123, 4654828689157546210, -6537336549913924143, -6069094837631746610, 5469373236997057060, 5030323985269958838, 1639311325811141713, 130748711215679279, 2766675954686809389, -6798232292191428265, -354275923469348383, -2309787250745817258, -8268318868076371617, -2092209193316978896, -4630127946343886205, -4472186268088715542, 1840478173032875491, -1156790491375795177, -3150599184806210345, 5756874849504675822, 6114559057922582442, 7745435167063129954, 3754482327632729208, 365803253680213997, 6119383719781273943, -5594863414894013370, -2694934598652091613, -929374096667997078, 6292952723283794099, 7337225928746838302, -6823818186247943001, -6727407037791462144, 5508806358313776183, -6519104993439511160, 3278379712460134274, -9146025503241097323, 2454803558974538718, 4200051851131271870, -5743413283798646470, 988192459132051233, 701763799927058024, -7544460416010441424, -2007739199162607588, 3099988747529733107, -4664066457796678469, 8306106175421864750, 8231892996369720287, 1078987995405460066, -641532435895731698, 5772201653853323008, -6708269968841425901, -2466030410535678273, 5937941237253981177, 7577260883644841675, -5094054617697758997, 7809325139696094840, -1166647206474532985, -8201554760111379069, -885347329958204757, 5292664922790765461, -6274898943203020668, -4881307015744127917, 3723812123242647430, -8190883809838087406, -4287795385591537303]\n"
     ]
    }
   ],
   "source": [
    "hashed = []\n",
    "for i in words:\n",
    "    hashed.append(hash(i))\n",
    "    \n",
    "print(hashed[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic counting collision, just to have an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in hashed:\n",
    "    counter +=  hashed.count(i)\n",
    "    \n",
    "# since all elements are already in the array\n",
    "print(counter - len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette methode est bien evidement pas la plus optimale mais tres pratique, pour word2.txt, cela prends quelques minutes sur google colab (j'ai chang√© l'allocation GPU pour aller plus vite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of crouse this hash function is efficient it is a build-in one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic hash : coding each letter by its in the alphabet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "LETTERS = {letter: str(index) for index, letter in enumerate(ascii_lowercase, start=1)} \n",
    "\n",
    "def alphabet_position(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    numbers = [LETTERS[character] for character in text if character in LETTERS]\n",
    "\n",
    "    return ' '.join(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandoner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 2 1 14 4 15 14 5 18'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[50])\n",
    "alphabet_position(words[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1 1', '1 1 12', '1 1 12 9 9', '1 1 13', '1 1 14 9', '1 1 18 4 22 1 18 11', '1 1 18 4 23 15 12 6', '1 1 18 15 14', '1 1 18 15 14 9 3', '1 1 18 15 14 9 3 1 12', '1 1 18 15 14 9 20 5', '1 1 18 15 14 9 20 9 3', '1 1 18 21', '1 2', '1 2 1', '1 2 1 2 4 5 8', '1 2 1 2 21 1', '1 2 1 3', '1 2 1 3 1', '1 2 1 3 1 20 5', '1 2 1 3 1 25', '1 2 1 3 9 14 1 20 5', '1 2 1 3 9 14 1 20 9 15 14', '1 2 1 3 9 19 3 21 19', '1 2 1 3 9 19 20', '1 2 1 3 11', '1 2 1 3 20 9 14 1 12', '1 2 1 3 20 9 14 1 12 12 25', '1 2 1 3 20 9 15 14', '1 2 1 3 20 15 18', '1 2 1 3 21 12 21 19', '1 2 1 3 21 19', '1 2 1 4 9 20 5', '1 2 1 6 6', '1 2 1 6 20', '1 2 1 9 19 1 14 3 5', '1 2 1 9 19 5 18', '1 2 1 9 19 19 5 4', '1 2 1 12 9 5 14 1 20 5', '1 2 1 12 9 5 14 1 20 9 15 14', '1 2 1 12 15 14 5', '1 2 1 13 1', '1 2 1 13 16 5 18 5', '1 2 1 14 4 15 14', '1 2 1 14 4 15 14 1 2 12 5', '1 2 1 14 4 15 14 5 4', '1 2 1 14 4 15 14 5 4 12 25', '1 2 1 14 4 15 14 5 5', '1 2 1 14 4 15 14 5 18', '1 2 1 14 4 15 14 13 5 14 20', '1 2 1 14 9 3', '1 2 1 14 20 5 19', '1 2 1 16 20 9 19 20 15 14', '1 2 1 18 1 13 2 15', '1 2 1 18 9 19', '1 2 1 18 20 8 18 15 19 9 19', '1 2 1 18 20 9 3 21 12 1 18', '1 2 1 18 20 9 3 21 12 1 20 9 15 14', '1 2 1 19', '1 2 1 19 5', '1 2 1 19 5 4', '1 2 1 19 5 4 12 25', '1 2 1 19 5 4 14 5 19 19', '1 2 1 19 5 13 5 14 20', '1 2 1 19 5 18', '1 2 1 19 7 9', '1 2 1 19 8', '1 2 1 19 8 5 4', '1 2 1 19 8 5 4 12 25', '1 2 1 19 8 5 4 14 5 19 19', '1 2 1 19 8 12 5 19 19', '1 2 1 19 8 12 5 19 19 12 25', '1 2 1 19 8 13 5 14 20', '1 2 1 19 9 1', '1 2 1 19 9 3', '1 2 1 19 11', '1 2 1 19 19 9 14', '1 2 1 19 20 1 18 4 9 26 5', '1 2 1 20 1 2 12 5', '1 2 1 20 5', '1 2 1 20 5 13 5 14 20', '1 2 1 20 5 18', '1 2 1 20 9 19', '1 2 1 20 9 19 5 4', '1 2 1 20 15 14', '1 2 1 20 15 18', '1 2 1 20 20 15 9 18', '1 2 1 20 21 1', '1 2 1 20 21 18 5', '1 2 1 22 5', '1 2 1 24 9 1 12', '1 2 1 24 9 12 5', '1 2 1 26 5', '1 2 2', '1 2 2 1', '1 2 2 1 3 15 13 5 19', '1 2 2 1 3 25', '1 2 2 1 4 9 4 5']\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "\n",
    "for i in words:\n",
    "    encoded.append(alphabet_position(i))\n",
    "print(encoded[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This hash function that is at first sight a bit naive is efficient due to the fact that I write each letter on a float that I concatenate in a big string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to decode for a given number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_encode(number, alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "    \"\"\"Converts an integer to an alphabet equivilent\"\"\"\n",
    "    if not isinstance(number, (int, long)):\n",
    "        raise TypeError(\"number must be an integer\")\n",
    "\n",
    "    if 0 <= number - 1 < len(alphabet):\n",
    "        return alphabet[number - 1] \n",
    "\n",
    "    base = ''\n",
    "    while number != 0:\n",
    "        number, r = divmod(number, len(alphabet))\n",
    "        if r == 0:\n",
    "            number = number - 1\n",
    "        base = alphabet[r - 1] + base\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cette fonction de hashage est bien plus longue est pas forcement per'tinente, elle met bcp de temps pour word2.txt, pour la tester utiliser un .txt file plus petit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex way to hash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "\n",
    "hashed = []\n",
    "for i in words:\n",
    "    hashed.append(hashlib.sha256(i.encode('utf-8')).hexdigest())\n",
    "    \n",
    "print(hashed[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in hashed:\n",
    "    counter +=  hashed.count(i)\n",
    "    \n",
    "    \n",
    "hashed.sort()\n",
    "colision = 0\n",
    "for i in range(len(hashed)-1):\n",
    "    if hashed[i] == hashed[i+1]:\n",
    "        colision+=1\n",
    "        \n",
    "print(\"The number of colisions in this basic hash function is :\", colision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68447 collisions sur 235886 mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This hash function is too simple (more than 30% of colisions), a polynomial function could be a relevant alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to avoid colisions : it happens when two different words receive the same hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"colisions.png\",width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this diagram, you can see the key (a string) is run through a hash function which produces an integer 0-15 which is used as the index in a 16-element array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here it is the case for John Smith and Sandra Dee, they have the hash 01. Once we want to encode it they will receive the same key. We want to avoid this phenomenon as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have experienced hashing in Python lets get more in depth by building ou hashing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Building a hash function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour sa construction on part  de word2.txt file : le taux d‚Äôoccupation sera au maximum de 30%, donc si on note x la taille de la table de hachage on a pour word2.txt : (235 886)/x  ‚â§  30/100 soit : x vallant 786 287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole process of hashing to a file :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to re-read the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En effet pour des problemes de memoire variable il vaut mieux tout faire d'un coup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hashage polynomial et le plus simple et pertinent √† mettre en place :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(file):\n",
    "    with open( file , \"r\") as f:\n",
    "        counter = 0\n",
    "        for word in f:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "def hash_fct(file):\n",
    "    x = wordcount(file)\n",
    "    table = [0 for i in range(3*x)]\n",
    "    with open( file , \"r\") as f:\n",
    "        y = 0\n",
    "        for word in f:\n",
    "            h = (hachage1(word,3*x) + 3 * hachage2(word,3*x) + 2 * hachage3(word,3*x))%(3*x)\n",
    "            if table[h] != 0:\n",
    "                y = y+1\n",
    "            table[h] = word\n",
    "        print(\"\\n\")\n",
    "        print(\"We have found\", y, \"on\", x, \"words in this file!\")\n",
    "        print(\"\\n\")\n",
    "        return(0)\n",
    "            \n",
    "def hachage1(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "    return (h%n)\n",
    "  \n",
    "def dec2bin(x):\n",
    "    return int(bin(x)[2:])\n",
    "\n",
    "def leftRotate(n, g):\n",
    "    INT_BITS = len(str(dec2bin(n)))\n",
    "    return (n << g)|(n >> (INT_BITS - g)) \n",
    "\n",
    "def hachage2(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "        h = h >> 1\n",
    "        h = h << 0\n",
    "    return (h%n)\n",
    "\n",
    "def hachage3(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "        h = leftRotate(h,5)\n",
    "    return (h%n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuite on passe tous nos .txt file dans une fonction pour que ca soit plus clair  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_word2():\n",
    "    print(\"Julien Pelegri hashing function for word2.txt\")\n",
    "    hash_fct(\"word2.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_word():\n",
    "    print(\"Julien Pelegri hashing function for word.txt\")\n",
    "    hash_fct(\"word.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_shakespeare():\n",
    "    print(\"Julien Pelegri hashing function for texte_Shakespeare.txt\")\n",
    "    hash_fct(\"texte_Shakespeare.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_corn():\n",
    "    print(\"Julien Pelegri hashing function for corncob_lowercase.txt\")\n",
    "    hash_fct(\"corncob_lowercase.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run the block above run the function pelegri_AAAA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for word.txt\n",
      "\n",
      "\n",
      "We have found 69 on 500 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environ 14% de colisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for word2.txt\n",
      "\n",
      "\n",
      "We have found 36273 on 235886 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_word2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environ 15% de colisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for texte_Shakespeare.txt\n",
      "\n",
      "\n",
      "We have found 3542 on 22906 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_shakespeare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environ 15% de colisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for corncob_lowercase.txt\n",
      "\n",
      "\n",
      "We have found 8961 on 58110 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_corn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environ 15% de colisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globalement on a 15% de colision, c'est pas l'id√©al et meme en complexifiant le hashage polynomial on arrive un peu √† baisser (vers 9%) mais ca rend beaucoup plus long le temps de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait des rotations √† gauche pour reduire le nombre colisions, on peut faire aussi a droite mais ca reduit que de peu les collisions pour un temps de calcul bcp plus important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On est sur plus de 15% de hashage mal classifi√©, les resultats ne sont pes tres bons, on se rend compte que des qu'on utilise pas une librairie python c'est difficile de faire quelque chose de pertinent si on veut garder un temps et un espace pertinent. C'etait neanmoins le but de l'exerice de ne pas utiliser des fonctions de python et de rester performant en temps et espace.\n",
    "\n",
    "### Plus on effectue d‚Äôop√©rations  sur les bits plus on reduit le nombre de colisions mais ensuite ca devient hyper long pour calculer la fonction de hashage ce qui n'est pas pratique du tout \n",
    "\n",
    "### Point fort : mes fonctions sont tres rapides meme sur des gros .txt file : maximun une minutes pour hash 500 000 mots\n",
    "\n",
    "### Pour la suite : ameliorer on hashage en complexifiant le nombre de degree de hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin & cryptocurrency hashing inspiration :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonction de hashage sont √† la base de la blockchain et de la s√©curit√© dans ce domaine, je voulais faire un petit test avec une fonciton de hashage basique deja dans python pour voir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilison SHA-256, la fonction de hashage du BTC (qui utilise du double hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file and counting it :\n",
      "['aaronic\\n', 'aaronical\\n', 'aaronite\\n', 'aaronitic\\n', 'aaru\\n', 'ab\\n', 'aba\\n', 'ababdeh\\n', 'ababua\\n', 'abac\\n', 'abaca\\n', 'abacate\\n', 'abacay\\n', 'abacinate\\n', 'abacination\\n', 'abaciscus\\n', 'abacist\\n', 'aback\\n', 'abactinal\\n', 'abactinally\\n']\n",
      "\n",
      "\n",
      "Now hashing it¬†:\n",
      "['8387af51d8e754377727ab6ddf90798d65c6668be744e0c1ec640500ea7d12a3', 'bdda1e7bd19f2be31417423437b416af57e6601fe832786afe544a4ae44d380d', '945df3003d42fffe3e1d58c4849b124355ae0a240173c62581c9e52268db5a0e', '9c90bc67e7359a65095c1ef6bd86ccba7a6746abdbc1a618001e962c2526b422', 'f259bd13ff818980d567e0bd579e2e684d342f21501479bdbf66b7309de0a848', 'a63d8014dba891345b30174df2b2a57efbb65b4f9f09b98f245d1b3192277ece', '3c31169062d772c4b37a5ac639c6c6d5c5632c66a1b7256a3470e8a313348d38', '0142856a3703cb31fa136f4153b6ae880e0e2a9500b83fe109d65006cde9bd4b', 'e6ed6e9a9289a2aaa37aab5f010adb97b39bd9e70048ff2bfc30f559675b3620', '4df3588453b520985be18a0531ed6a4ea1f8c3b925fffdf8384bb502bec47564', '4bbe7106ce8fab3afd8b870079d22abc2572180a150c26654e3f25bb1c0d7053', '1b803bc363cb145e7975100c035161c2d810b5bb61872ce66b84275f40f00524', 'ce017935bb36570317c249d9879ece0d46f299f5e55017a392ff5cc3637c4552', 'a25497ecc0c9b9bded8233ad1bb346bb1088e97eba614681e370a65fba10dd8d', 'ebb53ffd524a44b3a3aa06a0e916ff6acf097da880a92725533d14dbee6a5ed4', 'aea77e407ed0e015b74d78128676e1232ee62d33461be2c394761da0e4410065', '2ce3704b5c5573370f6343fcc7c4724eab43ef6b19fbe470eaaf16aa4d51d16e', '203242b7edd0062e44055cb75b8b8e796dabbb0f41fd80e53a14b1748a27f3cf', 'db2fc68f5543aacb5ab6fe4209fc49a17f634fc2fae63c45bb86383da79613d5', '744b9be45fb3d67130b688ae0fb538c8471f6d6f6c952b9a1982c6aef5ccfecc']\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def get_sha_256_hash(input_value):\n",
    "    return hashlib.sha256(input_value).hexdigest()\n",
    "\n",
    "def bitcoin(file):\n",
    "    words = []\n",
    "    print('reading file and counting it :')\n",
    "    with open( file , \"r\") as f:\n",
    "        for word in f:\n",
    "            words.append(word)\n",
    "    print(words[10:30])\n",
    "    print('\\n')\n",
    "    print('Now hashing it¬†:')\n",
    "    n = len(words)\n",
    "    hash_btc = []\n",
    "    for i in range(n):\n",
    "        hash_btc.append(get_sha_256_hash(words[i].encode('utf-8')))\n",
    "\n",
    "    print(hash_btc[10:30])\n",
    "    return(hash_btc)\n",
    "\n",
    "hash_bitcoin = bitcoin('word2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "235886\n",
      "\n",
      "\n",
      "The number of colisions using a hash function used in cryptocurrency : 1515\n",
      "\n",
      "\n",
      "With the complex hash function we have : 0.6422593964881341 % de colision, c'est tres faible.....\n"
     ]
    }
   ],
   "source": [
    "hash_bitcoin.sort()\n",
    "print(\"\\n\")\n",
    "print(len(hash_bitcoin))\n",
    "print(\"\\n\")\n",
    "\n",
    "colision = 0\n",
    "for i in range(len(hash_bitcoin)-1):\n",
    "    if hash_bitcoin[i] == hash_bitcoin[i+1]:\n",
    "        colision+=1\n",
    "        \n",
    "print(\"The number of colisions using a hash function used in cryptocurrency :\", colision)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"With the complex hash function we have :\", colision/len(hash_bitcoin)*100, \"% de colision, c'est tres faible.....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To see the evolution of the hash by doubling the hash : \n",
      "\n",
      "\n",
      "d7914fe546b684688bb95f4f888a92dfc680603a75f23eb823658031fff766d9\n",
      "2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824\n",
      "\n",
      "\n",
      "On voit deja la taille plus importante du hash\n",
      "\n",
      "\n",
      "False\n",
      "reading file and counting it :\n",
      "['abandons\\n', 'abase\\n', 'abased\\n', 'abasement\\n', 'abash\\n', 'abashed\\n', 'abate\\n', 'abated\\n', 'abatement\\n', 'abates\\n', 'abattoir\\n', 'abattoirs\\n', 'abbe\\n', 'abbess\\n', 'abbey\\n', 'abbeys\\n', 'abbot\\n', 'abbots\\n', 'abbreviate\\n', 'abbreviated\\n']\n",
      "\n",
      "\n",
      "Now hashing it¬†:\n",
      "['31b61fc3151b701836d0b3df49ba8f373b2c301c85cf91aa9a45f0e9c719eedc', 'a9d0f27bf59bf35d379ad8ab0271dddbcb51e5bcd46b319876f013b20e7502f6', '72d092c7461ece514fb7f6e508324cf6388b900b2f25283f4fb94cfa34b6fb05', '0148d9fb174d3772f2064ee7689afeb55ab30c950bb2fb852598b0dff67541e9', '9cef3f88d2299169bc8720fff9c8a7f02fa007a81db33f825f0005abf561c1aa', 'b4dcd73311a803d0c81e5c501defe90d622684e5d6b6ca7daa5aaaa5480508ad', 'ea8a94ec015ba7fbc9bae9fe07c70d10dd69770046d22f1075713ee17efbe6dc', 'b1b117c8130b401eb94a22d251db42934ca5d2b13c94f5e0b4e68bb74d554a42', '2411f3484523016d37682303994f4cac8d3a292f7c472130a71945de556ad53c', '1d20261b80c9fb8383609ca4156950d6c067a20c9f3757e0455d7bca8ad777c4', '39aac0c342010efcee44d6e8ee61b4de309be10369e35d4c81a2693052658d12', '84cea6b8d331b67a26eb6a80b35165af2ed18aea90ec2ed76f63fa800847d360', '69acdbc9b6ee4b8821c1ccad59abc161665a2550143e74b16b514071b533b107', 'b89561904528b6f0ee02fcd1ec03dd6499e61bc795817c3ef9854adf7fd8fc71', 'f78c9b0911d5645343688f7f549139b79bb091716024f80cef679d59c30e7870', 'f767dc96ef8eb2f7cf7c5a523a9ad118374e47c0068f9979bcf503a4d22b9090', '0baa5e4d8b9b0557fdfb20e388b720b62407d98c8c5052c8ab0f9328197c39dc', 'ae1def229b14e4fa83bc3640942700b96c21de320806853a1083dc4f23e96c9b', '33e34e9e1e1260c1c05bfb7af97785d698642ace1f288a6ae25fcf7aeff8a743', '9c11f8dcc0c16ef1873b80a1f5b7e52cea549d351e4aefd2c4348109429327ba']\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def get_sha_256_hash(input_value):\n",
    "    return hashlib.sha256(input_value).hexdigest()\n",
    "\n",
    "def double_btc_hash(a):\n",
    "    first_hash = get_sha_256_hash(a)\n",
    "    second_hash = get_sha_256_hash(first_hash.encode('utf-8'))       \n",
    "    return(second_hash)\n",
    "    \n",
    "print(\"To see the evolution of the hash by doubling the hash : \")\n",
    "print('\\n')\n",
    "print(double_btc_hash(\"hello\".encode('utf-8')))\n",
    "print(get_sha_256_hash('hello'.encode('utf-8')))\n",
    "print('\\n')\n",
    "print(\"On voit deja la taille plus importante du hash\")\n",
    "print('\\n')\n",
    "\n",
    "def bitcoin_2(file):\n",
    "    words = []\n",
    "    print('reading file and counting it :')\n",
    "    with open( file , \"r\") as f:\n",
    "        for word in f:\n",
    "            words.append(word)\n",
    "    print(words[10:30])\n",
    "    print('\\n')\n",
    "    print('Now hashing it¬†:')\n",
    "    n = len(words)\n",
    "    hash_btc_2 = []\n",
    "    for i in range(n):\n",
    "        hash_btc_2.append(double_btc_hash(words[i].encode('utf-8')))\n",
    "\n",
    "    print(hash_btc_2[10:30])\n",
    "    return(hash_btc_2)\n",
    "\n",
    "print(hash_bitcoin == hash_bitcoin_2)\n",
    "\n",
    "hash_bitcoin_2 = bitcoin_2('corncob_lowercase.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "58110\n",
      "\n",
      "\n",
      "The number of colisions using a hash function used in cryptocurrency : 1\n",
      "\n",
      "\n",
      "With the complex hash function we have : 0.0017208742040956805 % de colision, c'est tres faible.....\n"
     ]
    }
   ],
   "source": [
    "hash_bitcoin_2.sort()\n",
    "print(\"\\n\")\n",
    "print(len(hash_bitcoin_2))\n",
    "print(\"\\n\")\n",
    "\n",
    "colision = 0\n",
    "for i in range(len(hash_bitcoin_2)-1):\n",
    "    if hash_bitcoin_2[i] == hash_bitcoin_2[i+1]:\n",
    "        colision+=1\n",
    "        \n",
    "print(\"The number of colisions using a hash function used in cryptocurrency :\", colision)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"With the complex hash function we have :\", colision/len(hash_bitcoin_2)*100, \"% de colision, c'est tres faible.....\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction hashage de Monsieur Lemasquerier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will test it on the word.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de hachage simple\n",
    "def lemasquerier_bon_hashage(my_str, size):\n",
    "    current=size\n",
    "    n=len(my_str)\n",
    "    toggle = True\n",
    "    for x in range(len(my_str)):\n",
    "        if toggle:\n",
    "            current+=ord(my_str[x])\n",
    "        else:\n",
    "            current*=ord(my_str[x])\n",
    "        toggle = not toggle\n",
    "    current = (current+n)%size\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting my code to Lemasquerier hash function to pass my string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_reader(file):\n",
    "    words = []\n",
    "    print('reading file and counting it :')\n",
    "    with open( file , \"r\") as f:\n",
    "        for word in f:\n",
    "            words.append(word)\n",
    "    print(words[10:100])\n",
    "    print('\\n')\n",
    "    print('Now hashing it¬†:')\n",
    "    n = len(words)\n",
    "    hash_max = []\n",
    "    for i in range(n):\n",
    "        hash_max.append(lemasquerier_bon_hashage(words[i], n))\n",
    "\n",
    "    print(hash_max[10:100])\n",
    "    return(hash_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file and counting it :\n",
      "['aaronic\\n', 'aaronical\\n', 'aaronite\\n', 'aaronitic\\n', 'aaru\\n', 'ab\\n', 'aba\\n', 'ababdeh\\n', 'ababua\\n', 'abac\\n', 'abaca\\n', 'abacate\\n', 'abacay\\n', 'abacinate\\n', 'abacination\\n', 'abaciscus\\n', 'abacist\\n', 'aback\\n', 'abactinal\\n', 'abactinally\\n', 'abaction\\n', 'abactor\\n', 'abaculus\\n', 'abacus\\n', 'abadite\\n', 'abaff\\n', 'abaft\\n', 'abaisance\\n', 'abaiser\\n', 'abaissed\\n', 'abalienate\\n', 'abalienation\\n', 'abalone\\n', 'abama\\n', 'abampere\\n', 'abandon\\n', 'abandonable\\n', 'abandoned\\n', 'abandonedly\\n', 'abandonee\\n', 'abandoner\\n', 'abandonment\\n', 'abanic\\n', 'abantes\\n', 'abaptiston\\n', 'abarambo\\n', 'abaris\\n', 'abarthrosis\\n', 'abarticular\\n', 'abarticulation\\n', 'abas\\n', 'abase\\n', 'abased\\n', 'abasedly\\n', 'abasedness\\n', 'abasement\\n', 'abaser\\n', 'abasgi\\n', 'abash\\n', 'abashed\\n', 'abashedly\\n', 'abashedness\\n', 'abashless\\n', 'abashlessly\\n', 'abashment\\n', 'abasia\\n', 'abasic\\n', 'abask\\n', 'abassin\\n', 'abastardize\\n', 'abatable\\n', 'abate\\n', 'abatement\\n', 'abater\\n', 'abatis\\n', 'abatised\\n', 'abaton\\n', 'abator\\n', 'abattoir\\n', 'abatua\\n', 'abature\\n', 'abave\\n', 'abaxial\\n', 'abaxile\\n', 'abaze\\n', 'abb\\n', 'abba\\n', 'abbacomes\\n', 'abbacy\\n', 'abbadide\\n']\n",
      "\n",
      "\n",
      "Now hashing it¬†:\n",
      "[178518, 96882, 35542, 127406, 170662, 9519, 96034, 222294, 9602, 7168, 72506, 155008, 169609, 145904, 159538, 112850, 91858, 72606, 9286, 59486, 229649, 49614, 198422, 128409, 217126, 124760, 124900, 183810, 195586, 91701, 97282, 148439, 214822, 89262, 21066, 49102, 184126, 5898, 165354, 5908, 6038, 213762, 89584, 99240, 123185, 53742, 180184, 159054, 176464, 31891, 160816, 193710, 49969, 160129, 24386, 89674, 179623, 147031, 193740, 225490, 57018, 131100, 20504, 91600, 213488, 39423, 125349, 193770, 68532, 24210, 101381, 53854, 129808, 94935, 30014, 179187, 121673, 96075, 122827, 28534, 24894, 10028, 28384, 128682, 158262, 96044, 223945, 40590, 216522, 4477]\n"
     ]
    }
   ],
   "source": [
    "hash_max = max_reader('word2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of colisions in Lemasquerier hash function is : 93611\n"
     ]
    }
   ],
   "source": [
    "hash_max.sort()\n",
    "colision = 0\n",
    "for i in range(len(hash_max)-1):\n",
    "    if hash_max[i] == hash_max[i+1]:\n",
    "        colision+=1\n",
    "        \n",
    "print(\"The number of colisions in Lemasquerier hash function is :\", colision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction longue √† calculer et avec beaucoup de colision, mais il a du am√©liorer sa fonction entre temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of Jupiter Notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bitcoin_Price_Prediction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
