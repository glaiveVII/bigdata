{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Julien Pelegri hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : play around with hash funcitons, build some and encode some .txt file. Try to optimize time and space complexity and have as few collision as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hash.png\",width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max size of words I can get :  9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "print(\"The max size of words I can get : \" , six.MAXSIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Hash test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for 181 is: 181\n",
      "Hash for 181.23 is: 530343892119126197\n",
      "Hash for Python is: 8433711297258875460\n"
     ]
    }
   ],
   "source": [
    "# hash for integer unchanged\n",
    "print('Hash for 181 is:', hash(181))\n",
    "\n",
    "# hash for decimal\n",
    "print('Hash for 181.23 is:',hash(181.23))\n",
    "\n",
    "# hash for string\n",
    "print('Hash for Python is:', hash('Python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the .txt file size with the wc command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J'ai cree un word.txt file that has only 500 words to test faster\n",
    "\n",
    "wc word.txt\n",
    "\n",
    ">> 500\n",
    "\n",
    "wc word2.txt\n",
    "\n",
    ">> 235886\n",
    "\n",
    "wc texte_Shakespeare.txt\n",
    "\n",
    ">> 22906\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file first "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have create a smaller .txt file with only a to go faster when testing since the file is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the file:word2.txt\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "inputFile = input(\"Enter the name of the file:\")\n",
    "\n",
    "f=open(inputFile, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='word2.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "print(f)\n",
    "txt = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now read the file line per line :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(file):\n",
    "    words =[] #Liste pour stocker les différentes lignes\n",
    "    # Use str.rstrip() to remove a trailing newline \n",
    "    countword = 0\n",
    "\n",
    "    for line in f:\n",
    "        countword += 1\n",
    "       # print(len(words))\n",
    "        words.append(line.rstrip(\"\\n\"))\n",
    "    save = words\n",
    "    return(countword, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = count(f)\n",
    "words = x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235886\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235886\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zoogeographic', 'zoogeographical', 'zoogeographically', 'zoogeography', 'zoogeological', 'zoogeologist', 'zoogeology', 'zoogloea', 'zoogloeal', 'zoogloeic', 'zoogonic', 'zoogonidium', 'zoogonous', 'zoogony', 'zoograft', 'zoografting', 'zoographer', 'zoographic', 'zoographical', 'zoographically', 'zoographist', 'zoography', 'zooid', 'zooidal', 'zooidiophilous', 'zooks', 'zoolater', 'zoolatria', 'zoolatrous', 'zoolatry', 'zoolite', 'zoolith', 'zoolithic', 'zoolitic', 'zoologer', 'zoologic', 'zoological', 'zoologically', 'zoologicoarchaeologist', 'zoologicobotanical', 'zoologist', 'zoologize', 'zoology', 'zoom', 'zoomagnetic', 'zoomagnetism', 'zoomancy', 'zoomania', 'zoomantic', 'zoomantist', 'zoomastigina', 'zoomastigoda', 'zoomechanical', 'zoomechanics', 'zoomelanin', 'zoometric', 'zoometry', 'zoomimetic', 'zoomimic', 'zoomorph', 'zoomorphic', 'zoomorphism', 'zoomorphize', 'zoomorphy', 'zoon', 'zoonal', 'zoonerythrin', 'zoonic', 'zoonist', 'zoonite', 'zoonitic', 'zoonomia', 'zoonomic', 'zoonomical', 'zoonomist', 'zoonomy', 'zoonosis', 'zoonosologist', 'zoonosology', 'zoonotic', 'zoons', 'zoonule', 'zoopaleontology', 'zoopantheon', 'zooparasite', 'zooparasitic', 'zoopathological', 'zoopathologist', 'zoopathology', 'zoopathy', 'zooperal', 'zooperist', 'zoopery', 'zoophaga', 'zoophagan', 'zoophagineae', 'zoophagous', 'zoopharmacological', 'zoopharmacy', 'zoophile']\n"
     ]
    }
   ],
   "source": [
    "print(words[235500:235600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x[1][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now going through words array and hash it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha3-384', 'blake2s', 'sha3_224', 'mdc2', 'sm3', 'shake128', 'sha3-224', 'blake2b', 'sha512-224', 'shake256', 'md4', 'sha3_512', 'sha1', 'sha3-256', 'sha384', 'sha256', 'md5', 'blake2s256', 'sha512-256', 'sha512', 'shake_128', 'shake_256', 'sha3-512', 'whirlpool', 'blake2b512', 'md5-sha1', 'sha224', 'ripemd160', 'sha3_256', 'sha3_384'}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "print(hashlib.algorithms_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic python hash : use the hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4171325285679281702, -4171325285679281702, 241257824951959063, 4836067789152381503, 3902140602386838048, -7474235662673059313, 9141986221852515719, 6225106884422236343, -4794784451985316634, 9189272364536265254, 5458349250009168514, -7053676826261117251, -460387931375362167, 8043536204027204652, 2175191675575533109, 563500484603246902, -4885731984279105941, -5316575686257715355, -3590467628694245037, 5920740019567254019, 4765273707415591764, -5699296906277035707, -4716505829752630546, -2794321411084985558, 7305931724651824063, 2433029802783938724, -8959206682406745050, -5282633866437803901, -8920583246873706963, -785746101362605489, 6032376243553454593, 6120749525202700992, 7699598248885906366, 3145877043908382890, -2912563321880091547, 5819924712330762123, 4654828689157546210, -6537336549913924143, -6069094837631746610, 5469373236997057060, 5030323985269958838, 1639311325811141713, 130748711215679279, 2766675954686809389, -6798232292191428265, -354275923469348383, -2309787250745817258, -8268318868076371617, -2092209193316978896, -4630127946343886205, -4472186268088715542, 1840478173032875491, -1156790491375795177, -3150599184806210345, 5756874849504675822, 6114559057922582442, 7745435167063129954, 3754482327632729208, 365803253680213997, 6119383719781273943, -5594863414894013370, -2694934598652091613, -929374096667997078, 6292952723283794099, 7337225928746838302, -6823818186247943001, -6727407037791462144, 5508806358313776183, -6519104993439511160, 3278379712460134274, -9146025503241097323, 2454803558974538718, 4200051851131271870, -5743413283798646470, 988192459132051233, 701763799927058024, -7544460416010441424, -2007739199162607588, 3099988747529733107, -4664066457796678469, 8306106175421864750, 8231892996369720287, 1078987995405460066, -641532435895731698, 5772201653853323008, -6708269968841425901, -2466030410535678273, 5937941237253981177, 7577260883644841675, -5094054617697758997, 7809325139696094840, -1166647206474532985, -8201554760111379069, -885347329958204757, 5292664922790765461, -6274898943203020668, -4881307015744127917, 3723812123242647430, -8190883809838087406, -4287795385591537303]\n"
     ]
    }
   ],
   "source": [
    "hashed = []\n",
    "for i in words:\n",
    "    hashed.append(hash(i))\n",
    "    \n",
    "print(hashed[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic counting collision, just to have an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in hashed:\n",
    "    counter +=  hashed.count(i)\n",
    "    \n",
    "# since all elements are already in the array\n",
    "print(counter - len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette methode est bien evidement pas la plus optimale mais tres pratique, pour word2.txt, cela prends quelques minutes sur google colab (j'ai changé l'allocation GPU pour aller plus vite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of crouse this hash function is efficient it is a build-in one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic hash : coding each letter by its in the alphabet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "\n",
    "LETTERS = {letter: str(index) for index, letter in enumerate(ascii_lowercase, start=1)} \n",
    "\n",
    "def alphabet_position(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    numbers = [LETTERS[character] for character in text if character in LETTERS]\n",
    "\n",
    "    return ' '.join(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandoner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 2 1 14 4 15 14 5 18'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[50])\n",
    "alphabet_position(words[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1 1', '1 1 12', '1 1 12 9 9', '1 1 13', '1 1 14 9', '1 1 18 4 22 1 18 11', '1 1 18 4 23 15 12 6', '1 1 18 15 14', '1 1 18 15 14 9 3', '1 1 18 15 14 9 3 1 12', '1 1 18 15 14 9 20 5', '1 1 18 15 14 9 20 9 3', '1 1 18 21', '1 2', '1 2 1', '1 2 1 2 4 5 8', '1 2 1 2 21 1', '1 2 1 3', '1 2 1 3 1', '1 2 1 3 1 20 5', '1 2 1 3 1 25', '1 2 1 3 9 14 1 20 5', '1 2 1 3 9 14 1 20 9 15 14', '1 2 1 3 9 19 3 21 19', '1 2 1 3 9 19 20', '1 2 1 3 11', '1 2 1 3 20 9 14 1 12', '1 2 1 3 20 9 14 1 12 12 25', '1 2 1 3 20 9 15 14', '1 2 1 3 20 15 18', '1 2 1 3 21 12 21 19', '1 2 1 3 21 19', '1 2 1 4 9 20 5', '1 2 1 6 6', '1 2 1 6 20', '1 2 1 9 19 1 14 3 5', '1 2 1 9 19 5 18', '1 2 1 9 19 19 5 4', '1 2 1 12 9 5 14 1 20 5', '1 2 1 12 9 5 14 1 20 9 15 14', '1 2 1 12 15 14 5', '1 2 1 13 1', '1 2 1 13 16 5 18 5', '1 2 1 14 4 15 14', '1 2 1 14 4 15 14 1 2 12 5', '1 2 1 14 4 15 14 5 4', '1 2 1 14 4 15 14 5 4 12 25', '1 2 1 14 4 15 14 5 5', '1 2 1 14 4 15 14 5 18', '1 2 1 14 4 15 14 13 5 14 20', '1 2 1 14 9 3', '1 2 1 14 20 5 19', '1 2 1 16 20 9 19 20 15 14', '1 2 1 18 1 13 2 15', '1 2 1 18 9 19', '1 2 1 18 20 8 18 15 19 9 19', '1 2 1 18 20 9 3 21 12 1 18', '1 2 1 18 20 9 3 21 12 1 20 9 15 14', '1 2 1 19', '1 2 1 19 5', '1 2 1 19 5 4', '1 2 1 19 5 4 12 25', '1 2 1 19 5 4 14 5 19 19', '1 2 1 19 5 13 5 14 20', '1 2 1 19 5 18', '1 2 1 19 7 9', '1 2 1 19 8', '1 2 1 19 8 5 4', '1 2 1 19 8 5 4 12 25', '1 2 1 19 8 5 4 14 5 19 19', '1 2 1 19 8 12 5 19 19', '1 2 1 19 8 12 5 19 19 12 25', '1 2 1 19 8 13 5 14 20', '1 2 1 19 9 1', '1 2 1 19 9 3', '1 2 1 19 11', '1 2 1 19 19 9 14', '1 2 1 19 20 1 18 4 9 26 5', '1 2 1 20 1 2 12 5', '1 2 1 20 5', '1 2 1 20 5 13 5 14 20', '1 2 1 20 5 18', '1 2 1 20 9 19', '1 2 1 20 9 19 5 4', '1 2 1 20 15 14', '1 2 1 20 15 18', '1 2 1 20 20 15 9 18', '1 2 1 20 21 1', '1 2 1 20 21 18 5', '1 2 1 22 5', '1 2 1 24 9 1 12', '1 2 1 24 9 12 5', '1 2 1 26 5', '1 2 2', '1 2 2 1', '1 2 2 1 3 15 13 5 19', '1 2 2 1 3 25', '1 2 2 1 4 9 4 5']\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "\n",
    "for i in words:\n",
    "    encoded.append(alphabet_position(i))\n",
    "print(encoded[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235886\n",
      "3030\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(len(encoded))\n",
    "for i in encoded:\n",
    "    counter +=  encoded.count(i)\n",
    "    \n",
    "# since all elements are already in the array\n",
    "print(counter - len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This hash function that is at first sight a bit naive is efficient due to the fact that I write each letter on a float that I concatenate in a big string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to decode for a given number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_encode(number, alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "    \"\"\"Converts an integer to an alphabet equivilent\"\"\"\n",
    "    if not isinstance(number, (int, long)):\n",
    "        raise TypeError(\"number must be an integer\")\n",
    "\n",
    "    if 0 <= number - 1 < len(alphabet):\n",
    "        return alphabet[number - 1] \n",
    "\n",
    "    base = ''\n",
    "    while number != 0:\n",
    "        number, r = divmod(number, len(alphabet))\n",
    "        if r == 0:\n",
    "            number = number - 1\n",
    "        base = alphabet[r - 1] + base\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cette fonction de hashage est bien plus longue est pas forcement per'tinente, elle met bcp de temps pour word2.txt, pour la tester utiliser un .txt file plus petit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex way to hash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "\n",
    "hashed = []\n",
    "for i in words:\n",
    "    hashed.append(hashlib.sha256(i.encode('utf-8')).hexdigest())\n",
    "    \n",
    "print(hashed[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in hashed:\n",
    "    counter +=  hashed.count(i)\n",
    "    \n",
    "# since all elements are already in the array\n",
    "print(counter - len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68447 collisions sur 235886 mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to avoid colisions : it happens when two different words receive the same hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"colisions.png\",width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this diagram, you can see the key (a string) is run through a hash function which produces an integer 0-15 which is used as the index in a 16-element array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here it is the case for John Smith and Sandra Dee, they have the hash 01. Once we want to encode it they will receive the same key. We want to avoid this phenomenon as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have experienced hashing in Python lets get more in depth by building ou hashing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole process of hashing to a file :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to re-read the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En effet pour des problemes de memoire variable il vaut mieux tout faire d'un coup '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(file):\n",
    "    with open( file , \"r\") as f:\n",
    "        counter = 0\n",
    "        for word in f:\n",
    "            counter += 1\n",
    "    return(counter)\n",
    "\n",
    "def hash_fct(file):\n",
    "    x = wordcount(file)\n",
    "    table = [0 for i in range(3*x)]\n",
    "    with open( file , \"r\") as f:\n",
    "        y = 0\n",
    "        for word in f:\n",
    "            h = (hachage1(word,3*x) + 3 * hachage2(word,3*x) + 2 * hachage3(word,3*x))%(3*x)\n",
    "            if table[h] != 0:\n",
    "                y = y+1\n",
    "            table[h] = word\n",
    "        print(\"\\n\")\n",
    "        print(\"We have found\", y, \"on\", x, \"words in this file!\")\n",
    "        print(\"\\n\")\n",
    "        return(0)\n",
    "            \n",
    "def hachage1(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "    return (h%n)\n",
    "  \n",
    "def dec2bin(x):\n",
    "    return int(bin(x)[2:])\n",
    "\n",
    "def leftRotate(n, g):\n",
    "    INT_BITS = len(str(dec2bin(n)))\n",
    "    return (n << g)|(n >> (INT_BITS - g)) \n",
    "\n",
    "def rightRotate(n, d):\n",
    "    INT_BITS = len(str(dec2bin(n)))\n",
    "    return (n >> d)|(n << (INT_BITS - d)) & 0xFFFFFFFF\n",
    "\n",
    "def hachage2(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "        h = h >> 1\n",
    "        h = h << 0\n",
    "    return (h%n)\n",
    "\n",
    "def hachage3(word,n):\n",
    "    i = 1\n",
    "    h = 0\n",
    "    l = len(word)\n",
    "    for car in word:\n",
    "        h = h+(26**(l-i))*(ord(car) - 96)\n",
    "        i += 1\n",
    "        h = leftRotate(h,5)\n",
    "    return (h%n)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_word2():\n",
    "    print(\"Julien Pelegri hashing function for word2.txt\")\n",
    "    hash_fct(\"word2.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_word():\n",
    "    print(\"Julien Pelegri hashing function for word.txt\")\n",
    "    hash_fct(\"word.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_shakespeare():\n",
    "    print(\"Julien Pelegri hashing function for texte_Shakespeare.txt\")\n",
    "    hash_fct(\"texte_Shakespeare.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelegri_corn():\n",
    "    print(\"Julien Pelegri hashing function for corncob_lowercase.txt\")\n",
    "    hash_fct(\"corncob_lowercase.txt\")\n",
    "    print(\"Fin hashage' et comptage collisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run the block above run the function pelegri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for word.txt\n",
      "\n",
      "\n",
      "We have found 69 on 500 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for word2.txt\n",
      "\n",
      "\n",
      "We have found 36273 on 235886 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_word2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for texte_Shakespeare.txt\n",
      "\n",
      "\n",
      "We have found 3542 on 22906 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julien Pelegri hashing function for corncob_lowercase.txt\n",
      "\n",
      "\n",
      "We have found 8961 on 58110 words in this file!\n",
      "\n",
      "\n",
      "Fin hashage' et comptage collisions\n"
     ]
    }
   ],
   "source": [
    "pelegri_corn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait des rotations à droite et à gauche pour reduire le nombre colisions (astuce toruvée sur stackoverflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C'est logique la taille des 'sur laquelle on hash est plus grande, fonction de hashage efficace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction hashage de Monsieur Lemasquerier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will test it on the word.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de hachage simple\n",
    "def lemasquerier_bon_hashage(my_str, size):\n",
    "    current=size\n",
    "    n=len(my_str)\n",
    "    toggle = True\n",
    "    for x in range(len(my_str)):\n",
    "        if toggle:\n",
    "            current+=ord(my_str[x])\n",
    "        else:\n",
    "            current*=ord(my_str[x])\n",
    "        toggle = not toggle\n",
    "    current = (current+n)%size\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting my code to Lemasquerier hash function to pass my string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98, 98, 411, 20, 395, 21, 499, 128, 442, 168, 221, 375, 339, 363, 195, 8, 106, 205, 473, 201, 299, 212, 80, 382, 468, 117, 353, 309, 192, 396, 368, 364, 343, 116, 88, 113, 127, 290, 51, 108, 209, 172, 458, 329, 161, 347, 36, 449, 152, 450, 463, 337, 71, 168, 220, 447, 411, 331, 62, 374, 349, 451, 106, 176, 385, 275, 350, 46, 454, 456, 122, 91, 319, 112, 245, 156, 56, 457, 417, 122, 326, 54, 245, 92, 101, 108, 496, 232, 334, 311, 18, 260, 220, 328, 172, 107, 92, 90, 133, 348, 208, 321, 457, 110, 344, 460, 191, 130, 301, 26, 165, 468, 267, 41, 433, 263, 372, 278, 319, 17, 88, 190, 395, 403, 178, 324, 242, 198, 476, 237, 170, 473, 142, 454, 256, 328, 492, 162, 79, 332, 156, 170, 199, 216, 345, 255, 444, 143, 288, 146, 166, 318, 490, 433, 306, 107, 273, 484, 122, 150, 386, 110, 493, 498, 279, 132, 391, 288, 204, 158, 60, 162, 123, 234, 388, 284, 284, 484, 18, 194, 117, 482, 108, 293, 398, 300, 80, 397, 136, 156, 392, 388, 220, 168, 303, 282, 280, 484, 416, 304, 331, 229, 422, 451, 347, 127, 463, 337, 208, 392, 176, 329, 79, 459, 195, 220, 226, 236, 122, 318, 282, 177, 206, 420, 210, 160, 154, 191, 215, 331, 343, 345, 441, 452, 252, 178, 333, 254, 265, 265, 393, 421, 100, 441, 416, 318, 104, 60, 271, 172, 360, 474, 424, 470, 475, 22, 292, 482, 370, 402, 491, 70, 402, 236, 158, 282, 258, 122, 470, 408, 250, 324, 182, 387, 310, 268, 372, 125, 193, 120, 369, 172, 320, 138, 320, 226, 24, 202, 139, 80, 381, 141, 399, 334, 454, 460, 378, 84, 67, 80, 418, 123, 253, 196, 435, 306, 186, 18, 138, 473, 493, 353, 260, 51, 75, 223, 418, 448, 133, 470, 322, 206, 391, 278, 244, 413, 118, 418, 430, 18, 298, 76, 201, 467, 170, 123, 172, 335, 4, 337, 380, 264, 135, 142, 8, 154, 54, 327, 370, 81, 216, 120, 306, 266, 306, 225, 326, 282, 398, 326, 342, 47, 390, 103, 92, 432, 339, 174, 355, 11, 213, 264, 99, 344, 442, 128, 12, 488, 9, 102, 6, 486, 156, 443, 406, 250, 496, 50, 350, 459, 461, 355, 435, 389, 287, 418, 266, 352, 96, 226, 7, 240, 207, 406, 122, 162, 156, 310, 81, 412, 310, 415, 331, 88, 14, 44, 194, 225, 248, 246, 361, 284, 193, 57, 311, 215, 6, 254, 267, 346, 337, 380, 80, 84, 3, 362, 261, 26, 226, 468, 201, 136, 19, 342, 106, 170, 78, 179, 241, 95, 208, 372, 108, 178, 422, 414, 214, 215, 356, 394, 206, 454, 426, 42, 355, 70, 363, 440, 119, 254, 123, 152, 398, 138, 378, 426, 48, 65, 160, 411, 352, 104, 366, 322, 424, 470, 458, 347, 268, 34, 282, 183, 285, 283, 470, 407, 7, 109, 432, 2]\n"
     ]
    }
   ],
   "source": [
    "n = len(words)\n",
    "hash_max = []\n",
    "for i in range(n):\n",
    "    hash_max.append(lemasquerier_bon_hashage(words[i], n))\n",
    "\n",
    "print(hash_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of colisions in Lemasquerier hash function is : 582\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in hash_max:\n",
    "    counter +=  hash_max.count(i)\n",
    "    \n",
    "# since all elements are already in the array\n",
    "print(\"The number of colisions in Lemasquerier hash function is :\", counter - len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of Jupiter Notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bitcoin_Price_Prediction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
